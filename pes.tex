\documentclass{article}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\title{Appunti di Probabilità e Statisticas}
\author{Giacomo Zanatta}
\renewcommand*\contentsname{Indice}
\usepackage{Sweave}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Probabilità elementare}
\subsection{Contare le probabilità}
Se abbiamo un esperimento con n$_1$ possibili scelte, e per ogni scelta dell'esperimento 1 ci sono n$_2$ possibili scelte, allora in totale ci saranno $n_1*n_2$ possibili scelte.\\
In generale, se abbiamo $i$ esperimenti, ciascuno con $n_i$ possibili scelte, allora esistono in totale 
\begin{equation}
  \prod\limits_{i=1}^n m_i
\end{equation}
possibili scelte.\\ 
\\
\emph{
  Esempio: una commissione parlamentare deve essere composta da un membro del partito A, che conta 10 rappresentanti, da un membro del partito B, che conta 15 rappresentanti, e da un membro del   partito C, che conta 2 rappresentanti. In totale quindi ci sono $10*15*2 = 300$ possibili   commissioni parlamentari.
}
\\
\subsection{Disposizioni}
Una disposizione di r elementi di un insieme composta da n elementi è una scelta ordinata di r elementi tra quegli n.\\
Possiamo distinguere le disposizioni in:
\begin{enumerate}
  \item Disposizioni con ripetizione: in questo caso, ogni elemento può essere scelto più di una volta. \\ Le disposizioni con ripetizione di n elementi presi r alla volta sono
  \begin{equation}
    \prod\limits_{i=1}^n n = n^r.
  \end{equation}
  \item Disposizioni semplici (senza ripetizione): le disposizioni semplici di n elementi presi r alla volta sono 
    \begin{equation}
      n*(n-1)*...*(n-r+1) = \frac{n!}{(n-r)!}
    \end{equation}
\end{enumerate}
\emph{
ESEMPI:
\begin{enumerate}
\item Quante parole di 2 lettere possiamo comporre con le lettere I,L,A ? Siamo nel caso delle disposizioni con ripetizione, perchè una lettera può essere ripetuta. Quindi, in totale, abbiamo $3^2 = 9$ possibili parole: II, IL, IA, LI, LL, LA, AI, AL, AA.\\
Se invece le parole devono avere tutte lettere diverse, siamo nel caso delle disposizioni semplici, quindi avremo $3*2 = 6$ possibili parole: IL, IA, LI, LA, AI, AL.
\item Un bit può assumere i valori 0 o 1. Un byte è una sequenza di 8 bit. Quanti byte ci sono?
$2^8$ possibili byte, in quanto in questo caso dobbiamo usare le disposizioni con ripetizione.
\end{enumerate}
}
Le disposizioni vengono normalmente usate quando bisogna effettuare un campionamento casuale da un'urna (ossia estrazione di palle da un'urna).\\
Se un'urna contiene n palle distinguibili e dobbiamo estrarre r palle con reintroduzione, le estrazioni possibili sono $n^r$ (disposizioni con ripetizione).\\
Se invece le palle vengono estratte senza reintroduzione, allora le estrazioni possibili sono $n*(n-1)*...*(n-r+1)$ (disposizioni semplici).
\subsection{Permutazioni}
Una permutazione è una disposizione semplice di n elementi presi n alla volta, e sono
\begin{equation}
  n*(n-1)*...*2*1 = n!
\end{equation}
Le permutazioni vengono usate ad esempio per trovare il numero di anagrammi di una parola.
Se abbiamo m elementi che si ripetono, rispettivamente, k$_1$, k$_2$, ..., k$_m$ volte (ad esempio, se dobbiamo trovare gli anagrammi della parola CASA, abbiamo l'elemento A ripetuto 2 volte) allora è necessario usre la formula 
\begin{equation}
\frac{n!}{k_1!k_2!...k_n!}
\end{equation}
\emph{
ESEMPI:
\begin{enumerate}
\item Trovare il numero di anagrammi della parola CIAO: la parola in questione è composta da 4 lettere, e gli anagrammi possibili saranno $4!=4*3*2*1=24$ 
\item Trovare il numero di anagrammi della parola GATTO: notare che, sebbene questa parola ha lo stesso numero di lettere della parola precedente, ha la lettere T ripetuta 2 volte. Quindi i possibili anagrammi non saranno 24, ma $\frac{4!}{2}=12$ 
\end{enumerate}
}
\subsection{Combinazioni}
Un sottoinsieme di numerosità r scelto da un insieme con n elementi si chiama combinazione di n elementi presi r alla volta.
Il numero di combinazioni di n elementi presi r alla volta (con r <= n) è
\begin{equation}
  \frac{n(n-1)...(n-r+1)}{r!} = {{n}\choose{r}}
\end{equation}
Le combinazioni vengono utilizzate quando non è importante l'ordine (un insieme infatti non è ordinato).
\paragraph{Coefficente binomiale\\}
${{n}\choose{r}}$ si chiama coefficente binomiale, e si calcola mediante la formula
\begin{equation}
{{n}\choose{r}} = \frac{n!}{k!(n-r)!}
\end{equation}
La formula scritta precedentemente è riferita alle combinazioni semplici (senza ripetizione).\\
Se vogliamo contare il numero di sottoinsiemi tenendo conto anche di elementi ripetuti (quindi nel caso in cui ogni sottoinsieme, composto da r elementi, può contenere r volte lo stesso elemento), usiamo la formula
\begin{equation}
  {{n+r-1}\choose{r}} = \frac{(n+r-1)!}{r!(n-1)!}
\end{equation}
In questo caso, r può essere anche maggiore di n.
\subsection{Fenomeni aleatori}
Il calcolo delle probabilità è la logica dell'incerto.\\
La probabilità viene usata per ragionare sui possibili risultati di un fenomeno aleatorio.\\
Di un fenomeno aleatorio non si può prevedere con certezza l'esito.
Alcuni esempi di fenomeni aleatori sono:
\begin{enumerate}
\item Lancio di un dado
\item Lancio di una moneta 4 volte
\item L'estrazione di una mano di poker.
\end{enumerate}
\subsubsection{Spazio campionario, risultati, eventi}
\begin{enumerate}
\item Lo spazio campionario ($\Omega$) è un insieme che rappresenta i possibili risultati di un fenomeno aleatorio.
\item Un evento è un sottoinsieme dello spazio campionario ${A}\subset{\Omega}$.
\item I risultati ${\omega}$ sono detti eventi elementari. 
\item $\Omega$ è chiamato evento certo, in quanto si verifica sicuramente.
\end{enumerate}
Ad esempio, per il lancio di un dado, lo spazio campionario $\Omega$ è dato dall'insieme di tutti i possibili risultati, ossia $\Omega = \{1,2,3,4,5,6\}$, mentre un possibile evento può essere $A=\{5,6\}$ (evento risultato superiore a 4), oppure $B=\{2,4,6\}$ (evento risultato pari).
\subsubsection{Operazioni logiche sugli eventi}
\paragraph{Negazione (Complemento)\\}
Il complementare di un evento $A$, $\overline{A}$, è l'evento che è vero quando A è falso ed è falso quando A è vero.
L'evento impossibile è la negazione dell'evento certo: $\overline{\Omega} = \emptyset$
\paragraph{Intersezione\\}
L'intersezione di due eventi A e B, ${A}\cap {B}$, è l'evento che è vero quando sia A che B sono veri.
\paragraph{Unione\\}
L'unione di A e B, ${A}\cup{B}$, è l'evento che è vero quando A oppure B (oppure entrambi) sono veri.
\paragraph{Inclusione\\}
L'evento A è incluso nell'evento B se A è un sottoinsieme di B, ${A}\subset{B}$. In questo caso se A si verifica vuol dire che si è verificato anche B.
\paragraph{Incompatibilità\\}
Due eventi A e B si dicono incompatibili se ${A}\cap{B}=\emptyset$, ossia se non è possibile che entrambi siano veri.
\paragraph{Partizione\\}
Una partizione dello spazio campionario è una famiglia di eventi tale che ogni coppia di insiemi della famiglia ha intersezione vuota e l'unione di tutti i componenti della famiglia è lo spazio campionario $\Omega$.
\subsection{Definizione di Probabilità}
La probabilità una funzione degli eventi di uno spazio campionario, a valori in $R^+$, definita dai seguenti assiomi:
\begin{enumerate}
\item $0<=P(A)<=1$
\item $P(\Omega)=1$
\item Se ${A_{i}}\cap{{A}_j}=\emptyset,\forall{{i}\ne{j}}$ (sequenza di eventi incompatibili), allora
  \begin{equation}
  P(\bigcup_{n=1}^{\infty}A_n) = \sum\limits_{n=1}^{\infty}P(A_n)
  \end{equation}
\end{enumerate}
La probabilità di un evento $A$, definita come $P(A)$, è un numero tra 0 e 1 che indica il grado di fiducia del ricercatore nell'avverarsi dell'evento $A$. Se $P(A)=1$, allora l'evento si è avverato. Più $P(A)$ si avvicina ad 1, più ci aspettiamo che l'evento $A$ si avveri.
\subsubsection{Proprietà della probabilità}
\paragraph{Complementare\\}
\begin{equation}
  P(\overline{A}) = 1 - P(A)
\end{equation}
\paragraph{Evento impossibile\\}
\begin{equation}
  P(\emptyset) = P(\overline{\Omega}) = 1 - P(\Omega) = 0
\end{equation}
\paragraph{Unione\\}
\begin{equation}
  P({A}\cup{B}) = P(A) + P(B) - P({A}\cap{B})
\end{equation}
\paragraph{Partizione\\}
se $C_1,C_2,...$ sono partizioni, allora
\begin{equation}
  P(\bigcup_{i=1}^{\infty}C_i)) = P(\Omega) = 1
\end{equation}
\subsubsection{Spazi campionari finiti}
Se lo spazio campionario è un insieme finito ( $\Omega = \{\omega_1,...,\omega_n \}$ ), allora una assegnazione di probabilità è data da n valori $p_1, ...., p_n$ tali che:
\begin{enumerate}
\item $p_i \in [0,1]\; \forall{i=1,...n}$
\item $\bigcup_{i=1}^{n}p_i = 1$
\item $p_i = P(\{\omega_i\})\; \forall{i=1,...,n}$
\end{enumerate}
\subsubsection{Eventi elementari equiprobabili}
Se tutti gli eventi elementari hanno la stessa probabilità, allora
\begin{equation}
  p_i = P(\{\omega_i\}) = \frac{1}{n}, \; \forall{i=1,...,n}
\end{equation}
Quindi, per ogni evento $A$ (che ricordiamo è un'unione di eventi complementari, $A=\{\omega_{i1}, ..., \omega_{ir}\})$
\begin{equation}
  P(A)= \frac{r}{n} = \frac{\# A}{\# \Omega}
\end{equation}
ossia il numero di casi favorevoli fratto il numero di casi possibili.
\subsection{Popolazioni e sottopopolazioni}
Consideriamo ora una popolazione con N elementi suddivisi, a seconda che possiedono o meno una certa caratteristica, in due sottopopolazioni rispettivamente di K e N-K elementi.
La probabilità che su n elementi estratti casualmente esattamente k hanno la caratteristica è:
\begin{enumerate}
\item Con reinserimento:
\begin{equation}
P(A_k) = {{n}\choose{k}}(\frac{K}{N})^{k}(\frac{N-K}{N})^{n-k}
\end{equation}
\item Senza reinserimento (n<N):
\begin{equation}
P(A_k) = \frac{{{n}\choose{k}}{{N-K}\choose{n-k}}}{{{N}\choose{n}}}
\end{equation}
\end{enumerate}
\newpage
\section{Probabilità condizionata e composta}
\subsection{Probabilità condizionata}
La probabilità condizionata dell'evento $A$ dato l'evento $B$ è
\begin{equation}
P(A|B) = \frac{P({A}\cap{B})}{P(B)},\; P(B)>0
\end{equation}
La probabilità condizionata rappresenta la probabilità che si verifichi $A$, sapendo che $B$ è si è verificato.\\
Il campo delle possibilità quindi si restringe ad un sottoinsieme di $\Omega$ (l'insieme ${B}\subset{\Omega}$)
\subsection{Probabilità composta}
Per ottenere la probabilità di un'intersezione è possibile usare la probabilità condizionata:
\begin{equation}
  P({A}\cap{B}) = P(A|B)P(B)
\end{equation}
Questa formula viene chiamata formula delle probabilità composte e si può generalizzare a qualsiasi numero di eventi $A_1,...,A_n$
\begin{equation}
  P({A_1}\cap{A_2}\cap{...}\cap{A_n}) = P(A_n|{A_1}\cap{...}\cap{A_{n-1}})...P(A_3|{A_1}\cap{A_2})P(A_1)
\end{equation}
\subsection{Eventi indipendenti}
Se
\begin{equation}
P(A|B) = P(A)
\end{equation}
allora gli eventi $A$ e $B$ sono indipendenti. Si ha quindi:
\begin{equation}
P({A}\cap{B}) = P(A)P(B)
\end{equation}
Più in generale: gli eventi $A_1,...,A_n$ di dicono indipendenti se, comunque si prendono k>1 di essi, si ha:
\begin{equation}
P({A_{i1}}\cap{A_{i2}}\cap{...}\cap{A_{ik}}) = P(A_{i1})P(A_{ik})
\end{equation}
Notare che eventi indipendenti ed eventi disgiunti non è la stessa cosa!
\subsection{Legge della probabilità totale}
Se $C_1,C_2,...$ sono una partizione dell'evento certo, la probabilità di un qualsiasi evento $A$ può essere scritta come (legge della probabilità totale):
\begin{equation}
P(A) = \sum\limits_{i}{ P({A}\cap{C_i})} = \sum\limits_{i}{ P({C_i})P(A|C_i)}
\end{equation}
\subsection{La formula di Bayes}
Sia data la partizione $C_1, C_2,...$ e tutti i suoi elementi abbiamo probabilità positiva. Sia $A$ un ulteriore evento, anch'esso con probabilità positiva. Allora
\begin{equation}
P(C_m|A) = \frac{P({C_m}\cap{A})}{P(A)} = \frac{P(A|C_m)P(C_m)}{\sum\limits_{i}{P(A|C_i)P(C_i)}}
\end{equation}
Il teorema di Bayes permette di aggiornare l'assegnazione di probabilità data a priori di certi eventi $C_m$, quando l'evento $A$ si è verificato.\\
Il risultato sono le nuove probabilità $P(C_m|A)$, dette a posteriori.
\subsection{Sistemi}
\subsubsection{Sistemi in serie}
Un sistema formato da n componenti separati si dice in serie se funziona quando tutti gli n componenti funzionano.\\
Supponiamo che i componenti si guastino in modo indipendente e che la probabilità che il componente i-esimo si guasti sia $p_i$. Sia  $A_i$ l'evento in cui il componente i-esimo funziona e $A$ l'evento in cui l'intero sistema funziona. Allora:\\
\begin{equation}
P(A) = \prod\limits_{i=1}^{n}P(A_i) = \prod\limits_{i=1}^{n}(1-p_i)
\end{equation}
\subsubsection{Sistemi in parallelo}
Un sistema formato da n componenti separati si dice in parallelo se funziona quando almeno uno degli n componenti funziona.\\
Supponiamo come prima che i componenti si guastino in modo indipendente, e che la probabilità di guasti del componente i-esimo sia $p_i$.\\
Diciamo che $Ai$ è l'evento in cui il componente i-esimo funziona, e $A$ l'evento in cui l'intero sistema funziona.\\
Allora:
\begin{equation}
P(A) = 1 - P(\overline{A}) = 1 - \prod\limits_{i=1}^{n}P(\overline{A_i}) = 1-\prod\limits_{i=1}^{n}(1-p_i)
\end{equation}
\subsection{Test diagnostici}
La frazione dei soggetti affetti da una certa  malattia in una popolazione si chiama prevalenza.\\
Consideriamo un test diagnostico per la malattia.\\
La sensitività di un test è la probabilità che il test, somministrato ad un malato, sia positivo.\\
La specificità di un test è la probabilità che  il test, somministrato ad un non malato, sia negativo.\\
La situazione ideale sarebbe sensitività=specificità = 1.\\
Normalmente la situazione ideale non è raggiungibile, di conseguenza la situazione reale sarà sensitività < 1 e specificità < 1\\
Vediamo meglio con un esempio.\\
Immaginiamo di somministrare un test diagnostico non perfetto a una persona estratta a caso dalla popolazione. Consideriamo gli eventi:
\begin{enumerate}
  \item  M: la persona estratta è malata.
  \item +: il test dà risultato positivo.
  \item -:il test dà risultato negativo.
  \item ${\overline{M}}\cap{+}$: il test dà un falso positivo.
  \item${M}\cap{-}$: il test dà un falso positivo.
\end{enumerate}
Abbiamo, allora:
\begin{enumerate}
  \item $P(M) = $ prevalenza
  \item $P(+|M) = $ sensitività
  \item $P(-|M) = $ specificità
\end{enumerate}
\begin{enumerate}
  \item Probabilità di un falso positivo:
  \begin{equation}
    P(\overline{M}\cap{+}) = P(\overline{M}+P(+|\overline{M})
  \end{equation}
  (1-prevalenza)*(1-specificità)
  \item Probabilità di un falso negativo:
  \begin{equation}
    P({M}\cap{-}) = P(M)P(-|M)
  \end{equation}
  prevalenza * (1-specificità)
\end{enumerate}
\subsubsection{Esempio di test diagnostico}
Si studi un nuovo test per l'HIV.
Sia:\\
prevalenza = P(HIV) = 0.001 la proporzione di HIV nella popolazione studiata.\\
Sia inoltre:\\
P(+|HIV) = 0.95 sensitività
P(-|$\overline{HIV}$) = 0.98 specificità
La probabilità di un falso positivo è:\\
$P(\overline{HIV}\cap{+}) = P(\overline{HIV}+P(+|\overline{HIV}) = (1-O.OO1)*(1-0.098) = 0.01998$\\
La probabilità di un falso negativo è:\\
$P({HIV}\cap{-} = P(HIV)P(-|HIV) = 0.001(1-0.95) = 0.00005$
Per trovare la probabilità P(+), possiamo usare la legge della probabilità totale:\\
$P(+) = P (+|HIV)P(HIV)+P(+|\overline{HIV})P(\overline{HIV}) = 0.95*0.001+(1-0.98)*(1-0.001) = 0.02093$\\
Possiamo invece usare la formula di Bayes per trovare P(HIV|+), ossia la probabilità dell'evento "persona malata dato positivo":\\
$P(HIV|+)=\frac{P(HIV)P(+|HIV)}{P(HIV)P(+|HIV)+P(\overline{HIV})P(+|\overline{HIV})}$
\subsection{Filtri per la posta elettronica}
Consideriamo un filtro automatico che blocca i messaggi di spam in arrivo in una casella di posta.\\
Denotiamo:\\
S = il messaggio è  di spam\\
W =  il messaggio contiene determinate parole chiave.\\
Grazie al teorema di Bayes possiamo fissare a priori la probabilità che un messaggio sia Spam e poi aggiornarla nel caso in cui il messaggio contenga le parole chiave considerate:\\
$P(S|W) = \frac{P(W|S)P(S)}{P(W|S)P(S)+P(W|\overline{S})P(\overline{S})}$
\newpage
\section{Variabili aleatorie}
Una variabile aleatoria X è una funzione che assume  valori numerici determinati dall'esito di un certo fenomeno aleatorio. Se $\Omega$ è lo spazio campionario del fenomeno interessato, allora X è una particolare funzione $X:\Omega->R$\\
\subsection{Spazio campionario indotto}
Una variabile aleatoria definisce un nuovo spazio campionario numerico, costituito da tutti i possibili valori assunti dalla variabile stessa.\\
Si passa, quindi, da un generico spazio campionario ad un sottoinsieme di R.\\
Esistono due tipi di variabili aleatorie (a seconda del sottoinsieme dello spazio campionario indotto):\\
\begin{enumerate}
  \item Variabili aleatorie discrete.\\
  Una v.a. discreta X assume valori in un insieme numerabile di punti ${x_1,x_2,...,x_i,...}$.
  Un modello probabilistico per X è un'assegnazione di probabilità ad ogni suo possibile valore:\\
  \begin{equation}
  P(X=x_i) = p_i, \; \forall{i=1,2,...}
  \end{equation}
Per calcolare P(${X}\in{A}$), si sommano le probabilità dei singoli valori che appartengono ad A.\\


  \item Variabili aleatorie continue.\\
  Una v.a. continua X assume valori in un insieme continuo di punti (sottoinsieme di R).\\
  Un modello probabilistico per X è un'assegnazione di probabilità ad ogni sottoinsieme di suoi possibili valori:\\
  \begin{equation}
    P({X}\in{A}) = area\; su\; A\; sottesa\; ad\; una\; curva
  \end{equation}
  La curva è il grafico di una funzione f(x), chiamata densità di probabilità.
  
\end{enumerate}
\subsection{Variabili aleatorie discrete}
\subsubsection{Funzione di probabilità}
Un'assegnazione di probabilità per X viene chiamata funzione di probabilità e può essere rappresentata tramite un diagramma a bastoncini.
\subsubsection{Valore atteso (media)}
La media di una v.a. X discreta è:
\begin{equation}
E(X) = \sum_{i}{x_{i}p_{i}}
\end{equation}
Sia Y una v.a. ottenuta trasformando la v.a. X tramite la funzione g:R->R. Il valore atteso di Y si può calcolare anche senza conoscerne direttamente la sua funzione di probabilità.
\begin{equation}
E(Y)=\sum_{i}g(x_i)p_i
\end{equation}
\subsubsection{Varianza}
La varianza di v.a. discreta è:
\begin{equation}
Var(X)=\sum_{i}{(x_{i}-E(X))^{2}p_i}
\end{equation}
Oppure (da comunque lo stesso risultato):
\begin{equation}
Var(X)=\sum_{i}{{{x_i}^{2}p_i}}-E(X)^2
\end{equation}
\subsection{Variabili aleatorie continue}
\subsubsection{Densità di probabilità}
Una densità di probabilità è una particolare funzione f(x) che possiede le seguenti caratteristiche:
\begin{itemize}
\item $f(x)>=0, \forall{x\in{R}}$
\item $\int_{R}{f(x)dx}=1$
\end{itemize}
Dopo aver assegnato una densità di probabilità, possiamo calcolare la probabilità di A (sottoinsieme di R) con:
\begin{equation}
P(X\in{a})=\int_{A}{f(x)dx}
\end{equation}
dove A è un evento di R. Notare che $P(X=x) = 0, \forall{x\in{R}}$
\subsubsection{Funzione di ripartizione}
È una funzione continua. Se X è una varaibile aleatoria continua con densità f(x), allora la sua funzione di ripartizione è:\\
\begin{equation}
F(x) = \int_{-\infty}^{x}f(t)dt
\end{equation}
Dalla funzione di ripartizione possiamo risalire alla densità di probabilità:
\begin{equation}
f(x) = \frac{dF(x)}{dx}
\end{equation}
\subsubsection{Valore atteso (media)}
Sia X una v.a. continua con densità f(x). Allora il valore atteso di X è
\begin{equation}
E(X) = \int_0^{\infty}{2xe^{-2x}dx = \frac{1}{2}}
\end{equation}
La media ha le seguenti proprietà:
\begin{itemize}
\item $E(a)=a$, con a costante.
\item $E(aX+b)=aE(X)+b$, dove a e b sono costanti.
\end{itemize}
Sia Y una v.a. ottenuta trasformando la v.a. X tramite la funzione g:R->R. Il valore atteso di Y si può calcolare anche senza conoscerne direttamente la sua funzione di probabilità.
\begin{equation}
E(Y)=\int_{R}g(x)f(x)dx
\end{equation}
\subsubsection{Varianza}
Sia X una v.a. continua, con densità f(x). Allora la varianza di X è:
\begin{equation}
Var(X)=\int_{R}{(x-E(X))^{2}F(x)dx}
\end{equation}
Oppure:
\begin{equation}
Var(X)=\int_{R}{x^{2}f(x)dx-E(X)^2}
\end{equation}
La varianza ha le seguenti proprietà:
\begin{itemize}
\item $Var(a) = a$, a costante.
\item $Var(aX+b)=a^{2}Var(X)$, a e b costanti.
\end{itemize}
\subsection{Moda, mediana, quantili di una v.a.}
La moda di una v.a. continua o discreta X è il punto (o i punti) in cui la funzione di probabilità/densità assume valore massimo. È un indice di posizione.\\
La mediana è il minimo valore m per cui
\begin{equation}
F(m)=P(X<=m)>=\frac{1}{2}
\end{equation}
Per una v.a. continua, la mediana è l'unico punto m in cui $F(m)=P(X<=m)=P(X>=m)=\frac{1}{2}$
I quantili sono indici di posizione che generalizzano il concetto di mediana di una distribuzione.
Fissato un valore ${a}\in{(0,1)}$, il quantile di livello a di una v.a. X è il minimo valore $q_a$ per cui
\begin{equation}
F(q_a)=P(X<=q_a)>=a
\end{equation}
Per una v.a. continua il quantile di livello a è l'unico punto $q_a$ per cui $F(q_a)=P(X<=q_a)=a$.
\subsection{Alcune variabili aleatorie discrete}
\subsubsection{Distribuzione uniforme}
Consideriamo una v.a. X che assume un numero finito di valori tutti con la stessa probabilità $p_i=1/n$.\\
Si dice allora che X ha una distribuzione uniforme e si scrive X$\sim U\{x_1,...,x_n\}$.
\subsubsection{Distribuzione ipergeometrica}
Sia X la v.a. che conta il numero di successi su n estrazioni senza reinserimento da una popolazione con N elementi dei quali K sono considerati successo.\\
Si dice che X ha una distribuzione ipergeometrica di parametri N,K e n e si scrive $X\sim Ip(N,K,n)$.\\
Abbiamo che:
\begin{equation}
P(X=k)=\frac{{{N}\choose{k}}{{N-K}\choose{n-k}}}{{{N}\choose{n}}}
\end{equation}
\begin{equation}
E(X)=n\frac{K}{N}
\end{equation}
\begin{equation}
Var(X)=n{\frac{K}{N}}{\frac{N-K}{N}}{\frac{N-n}{N-1}}
\end{equation}
\subsubsection{Distribuzione binomiale}
Sia X la v.a. che conta il numero di successi su n estrazioni con reinserimento da una popolazione con N elementi dei quali K sono considerati successo. La probabilità di successo rimane invariata ad ogni estrazione successiva, pari a p=K/N.\\
X ha una distribuzione binomiale di parametri $n,p \in{(0,1)}$ e si scrive $X\sim Bi(n,p)$.
\begin{equation}
P(X=k)={{n}\choose{k}}p^k(1-p)^{n-k}, k<=n
\end{equation}
\begin{equation}
E(X)=np
\end{equation}
\begin{equation}
Var(X)=np(1-p)
\end{equation}
\subsubsection{Distribuzione di Poisson}
Una variabile aleatoria X che assume valori nell'insieme dei numeri naturali N, ha distribuzione di Poisson di parametro $\lambda>0$ se
\begin{equation}
P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}
\end{equation}
Scriveremo quindi $X\sim P(\lambda)$.
\begin{equation}
E(x)=Var(X)=\lambda
\end{equation}
La v.a. di Poisson viene utilizzata come modello per il conteggio di manifestazioni di un certo fenomeno di interesse, come ad esempio:
\begin{enumerate}
\item chiamate in arrivo ad un centralino in un certo intervallo di tempo
\item macchine transitanti ad un casello autostradale in un certo periodo del giorno
\item difetti rilevati in un pezzo di filo d'acciao prodotto da una ditta
\item terremoti manifestatisi in una data area nell'arco degli ultimi 10 anni
\end{enumerate}
In una binomiale, quando n>=100 e p<=0.05, possiamo approssimare la v.a. binomiale ad una poisson, con $\lambda=np$.
\subsubsection{Distribuzione geometrica}
Sia X una v.a. che conta il numero di ripetizioni indipendenti necessarie per osservare il primo successo in un esperimento binario che ha probabilità di successo p.\\
Diciamo che X ha una distribuzione geometrice di parametro $p\in (0,1)$ e si scrive $X\sim Ge(p)$:
\begin{equation}
P(X=k)=(1-p)^{k-1}p
\end{equation}
\begin{equation}
E(X)=\frac{1}{p}
\end{equation}
\begin{equation}
Var(X)=\frac{1-p}{p^2}
\end{equation}
La distribuzione geometrica ha la proprietà di mancanza di memoria. Questo significa che $P(X>m+n|X>m)=P(X>n)$.
\subsection{Alcune variabili aleatorie continue}
\end{document}